{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"DP_Tuts/Tut3-1/","title":"Tut3 1","text":""},{"location":"DP_Tuts/Tut3-1/#problem-statement","title":"Problem Statement","text":"<p>You are given an array of size n containing integers. Design a linear time algorithm to find the contiguous sub-array such that the sum of elements is as large as possible.</p>"},{"location":"DP_Tuts/Tut3-1/#recurrence-relation","title":"Recurrence Relation","text":"<p>\\(a_i\\) := the i'th element of the array \\(S_i\\) := sub-sequence of largest value ending at \\(i\\), including \\(a_i\\) \\(\\Omega_i\\) := sum of all elements in \\(S_i\\)</p> <p>Suppose we are at \\(a_i\\). Consider the sub-sequence \\(S_i\\). If \\(\\Omega_{i-1}\\) is positive, then  </p> \\[ \\quad \\Omega_i = \\Omega_{i-1} + a_i \\] <p>Otherwise,</p> \\[ \\quad \\Omega_{i} = a_i \\] <p>This gives the general value of \\(\\Omega_i\\) as</p> \\[ \\quad \\Omega_i = max{     \\begin {cases}         \\Omega_{i-1} + a_i &amp; \\\\         a_i &amp; \\\\     \\end {cases} } \\] <p>Since the largest sub-sequence possible must end at some position, we can find it with the expression,</p> \\[\\quad \\Omega = max(\\Omega_i)\\]"},{"location":"DP_Tuts/Tut3-1/#recursive-algorithm","title":"Recursive Algorithm","text":"<p>Base Case:  * for i = 0, \\(\\Omega_i\\) = 0</p> <p>Recursive Case: * \\(\\Omega_i = max(\\Omega_{i-1} + a_i, a_i)\\)</p> <p>Starting Point: * \\(\\Omega_n\\)</p> <p>Alternatively, we can perform this iteratively by calculating all \\(\\Omega_i\\) from 1 to n in that order.</p>"},{"location":"DP_Tuts/Tut3-1/#complexity","title":"Complexity","text":"<p>This algorithm performs \\(n\\) passes to calculate each value of \\(\\Omega_i\\) , then performs \\(max\\) operation in \\(n-1\\) comparisons. This gives us a time complexity of \\(O(n)\\), i.e. linear.</p> <p>Additionally, it has space complexity of \\(O(n)\\) with a naive approach, that can be optimised to \\(O(1)\\), by storing (\\(\\Omega_i\\) , \\(\\Omega_{i-1}\\) , and \\(\\Omega_{max}\\) only)</p>"},{"location":"DP_Tuts/Tut3-1/#runtime-implementation","title":"Runtime Implementation","text":"<p>Let a be \\(1, 2, 5, -1, -3, -20, 4, 2, 1\\) </p> i 1 2 3 4 5 6 7 8 9 \\(\\Omega_i\\) 1 3 8 7 4 -16 4 6 7 <p>\\(max(\\Omega_i)\\) = 8</p>"},{"location":"DP_Tuts/Tut3-2/","title":"Tut3 2","text":""},{"location":"DP_Tuts/Tut3-2/#problem-statement","title":"Problem Statement","text":"<p>You are given an array of size n containing integers. Design a linear time algorithm to find the sub-array (not necessarily contiguous) of maximum length such the values are non-decreasing.</p>"},{"location":"DP_Tuts/Tut3-2/#problem-definition","title":"Problem Definition","text":"<p>\\(a_i\\) := \\(i'th\\) number in the array \\(S_i\\) := the non-decreasing sub-array of maximum length ending at \\(i\\), including it \\(\\Omega_i\\) := the length of \\(S_i\\)</p>"},{"location":"DP_Tuts/Tut3-2/#recurrence-relation","title":"Recurrence Relation","text":"<p>Stand at \\(a_i\\) . The subsequence ending at \\(i\\) either connects to a previous sub-array, or starts a new one. If it extends from a previous sub-sequence, it must extend from the longest previous sub-sequence. Otherwise, the length is just 1.</p>"},{"location":"DP_Tuts/Tut3-2/#recursive-algorithm","title":"Recursive Algorithm","text":"<p>Base Case: * \\(i\\) = 0, \\(\\Omega_i\\) = 0 {there are no elements in the sub-array}</p> <p>Recursive Case:</p> \\[ \\begin {align} \\quad &amp; K \\in k,\\; \\Omega_K = max(\\Omega_k)\\\\  &amp;\\Omega_i =      \\begin {cases}         \\Omega_{K} + 1 &amp;|\\quad \\exists \\; k &lt; i \\; :a_i &gt;= a_{k} \\\\         1 &amp;|\\quad \\nexists \\; k &lt; i \\; :  a_i &gt;= a_{k}     \\end {cases} \\end {align} \\] <p>Going back to search for the ideal \\(k\\) will be \\(O(n)\\) in the worst case, giving us a complexity of \\(O(n^2)\\)</p>"},{"location":"DP_Tuts/Tut3-2/#runtime-analysis","title":"Runtime Analysis","text":"i 1 2 3 6 2 1 5 9 \\(\\Omega_i\\) 1 2 3 4 3 2 4 5 ___"},{"location":"DP_Tuts/Tut4-1/","title":"Tut4 1","text":""},{"location":"DP_Tuts/Tut4-1/#problem-statement","title":"Problem Statement","text":"<p>Given two strings X = x1, x2, x3, \u00b7 \u00b7 \u00b7 xm and Y = y1, y2, \u00b7 \u00b7 \u00b7 yn, find a common subsequence (not necessarily contiguous) of X,Y that is of the longest possible length.</p>"},{"location":"DP_Tuts/Tut4-1/#problem-definition","title":"Problem Definition","text":"<p>\\(X:= x_1, x_2, ... x_m\\) ,  \\(Y := y_1, y_2, ... y_n\\) </p>"},{"location":"DP_Tuts/Tut4-1/#recurrence-relation","title":"Recurrence Relation","text":"<p>Say we know the longest shared subsequence ending at some \\(x_i\\) and \\(y_j\\), not necessarily including those.</p> <p>Say the last two letters are overlapping. In this scenario \\(\\quad\\Omega_{i,\\, j} = \\Omega_{i-1, \\, j-1} + 1\\)</p> <p>Consider  ABCDEFG BCDEFGH \\(\\quad\\Omega_{i,\\,j} = \\Omega_{i,\\,j-1}\\)</p> <p>if the two strings were interchanged, it would come as \\(\\quad\\Omega_{i, j} = \\Omega_{i-1,\\,j}\\)</p> <p>So, we can generalise this algorithm as </p> \\[ \\Omega_{i,j} = max     \\begin {cases}         \\Omega_{i-1,\\,j-1} + 1 &amp; \\text{}\\\\         \\Omega_{i-1,\\,j} &amp; \\\\         \\Omega_{i,\\,j-1} &amp; \\\\     \\end {cases} \\] - A B C F G H I F 0 0 0 1 0 0 0 G 0 0 0 0 2 0 0 H 0 0 0 0 0 3 0 I 0 0 0 0 0 0 4 A 1 0 0 0 0 0 0 B 0 2 0 0 0 0 0 C 0 0 3 0 0 0 0"},{"location":"Divide%20and%20Conquer/Inversion%20Counting/","title":"Inversion Counting","text":"<p>Consider the sorted array,</p> \\[[1, 2, 3, 4, 5, 6, 7, 8]\\] <p>and the unsorted array,</p> <p>\\(\\([4, 3, 5, 2, 7, 6, 8, 1]\\)\\) The pair \\((4,3)\\) is out of order in the unsorted array. Let us call this out-of-order-pair as an inversion. </p>"},{"location":"Divide%20and%20Conquer/Inversion%20Counting/#motivation","title":"Motivation","text":"<p>Why do we care about the number of inversions? Suppose we ask two people to give ratings to elements in a set. This is done often with movie or show review websites, but can also be applied to other things like elections. </p>"},{"location":"Divide%20and%20Conquer/Inversion%20Counting/#naive-approach","title":"Naive Approach","text":"<p>A naive approach would look at every pair of elements from \\(1\\) to \\(n\\). It will then check if this \\((i, j)\\) pair is inverted or not. There will be \\(nC2\\) such pairs, giving us a complexity of \\(O(n^2)\\)</p>"},{"location":"Divide%20and%20Conquer/Inversion%20Counting/#divide-and-conquer-approach","title":"Divide-and-Conquer Approach","text":"<p>The key observation here is:</p> <p>Suppose we partition the unsorted array \\(A\\) of length \\(n\\) into two partitions \\(X\\) and \\(Y\\). We can find the inversions of the format \\((X_i, Y_j)\\) in \\(O(n)\\) time. These can be referred to as cross-inversions or split-inversions, provided that they are sorted. How is this possible?</p> <p>Say \\(X = [3, 4, 6]\\) and \\(Y = [1, 2, 5]\\). A mergesort-style algorithm will combine these two into \\(A\\). We start by picking \\(1\\) from \\(Y\\), but that immediately tells us that it's smaller than every element in \\(X\\). So we know there are \\(3\\) inversions here {\\((3, 1), (4, 2), (6, 2)\\)}. In fact, every time we pick an element from \\(Y\\), we know it is smaller than every element remaining in \\(X\\) {call this \\(x\\)}. So, every time we pick an element from \\(Y\\), we add \\(x\\) inversions to the total. If an element is picked from \\(X\\), the natural order has been maintained. </p>"},{"location":"Divide%20and%20Conquer/Inversion%20Counting/#recurrence-relation","title":"Recurrence Relation","text":"<p>The subproblems are divided as: -&gt; Sort and count inversions to the left of the partition -&gt; Sort and count inversions to the right of the partition -&gt; Count the number of split inversions, add all three values, merge the array, and return that.</p> <p>This gives us a Recurrence relation of</p> \\[T(0, n) = 2T(n/2) + O(n)\\] <p>Because this is a tiny modification of MergeSort, it comes to a complexity of \\(O(n.logn)\\) as well</p>"},{"location":"Divide%20and%20Conquer/Inversion%20Counting/#pseudo-code","title":"Pseudo-Code","text":"\\[CountAndMerge(X, Y)\\] <pre><code>toret = []\nfor j, k in X, Y:\n    if j &lt;= k:\n        append j to toret\n        move head of X ahead\n    else:\n        append k to toret\n        move head of Y ahead\n        count += len(X, j onwards)\nreturn (toret, count)\n</code></pre> \\[SortAndCountInv(A)\\] <pre><code>if len(A) == 1:\n    return 0   &lt;- base case\n\nX = first half of A\nY = second half of A\n\n(B, x) = SortAndCountInv (X)\n(C, y) = SortAndCountInv (Y)\n(D, z) = CountAndMerge(B, C)\nreturn (D, x+y+z)\n</code></pre>"},{"location":"Divide%20and%20Conquer/Minimum%20Distances/","title":"Minimum Distances","text":"<p>This algorithm involves finding the closest point pair in a lattice of \\(n\\) points.</p>"},{"location":"Divide%20and%20Conquer/Minimum%20Distances/#motivation","title":"Motivation","text":"<p>This algorithm can be used to find the closest entity to a user, whether it is a cab or a restaurant, by knowing the locations of both bodies. </p>"},{"location":"Divide%20and%20Conquer/Minimum%20Distances/#1-d-case","title":"1-D Case","text":"<p>In 1 dimension, there's a straightforward solution to this. We can sort the points and measure the distance between each point and its adjacent point, while storing the minimum distance so far. This will give us an easy way to get the minimum distance between any pair.</p>"},{"location":"Divide%20and%20Conquer/Minimum%20Distances/#2-d","title":"2-D","text":"<p>Unfortunately such an algorithm won't work in 2D. An alternative approach helps us though.</p> <p>For the array \\(P\\), let \\(P_x\\) be \\(P\\) sorted by the x co-ordinate, and \\(P_y\\) be \\(P\\) sorted by the y co-ordinate. </p> <p>The next step is to divide \\(P_x\\) by its median. Since these points are sorted, the median can be derived in \\(O(1)\\) time.</p>"},{"location":"Divide%20and%20Conquer/Minimum%20Distances/#divide-and-conquer","title":"Divide and Conquer","text":"<p>The median divides \\(P_x\\) into \\(Q_x\\) and \\(R_x\\), and \\(P_y\\) into \\(Q_y\\) and \\(R_y\\). \\(Q_x\\) and \\(R_x\\) can be computed in \\(O(1)\\), \\(Q_y\\) and \\(R_y\\) in \\(O(n)\\) time. </p> \\[ClosestPair(P_x, P_y)\\] <p>can be defined as </p> <pre><code>ClosestOf(\n    ClosestPair(Q_x, Q_y),\n    ClosestPair(Q_x, Q_y),\n    ClosestSplitPair(Q_x, Q_y, R_x, R_y)\n)\n</code></pre> <p>The first two can be found recursively, the last one however, is more complex.</p>"},{"location":"Divide%20and%20Conquer/Minimum%20Distances/#closest-split-pair","title":"Closest Split Pair","text":"<p>The closest split pair can be found by such. Call the minimum distance of closest pair on either side as \\(\\delta\\). Then look at all points within a \\(\\delta\\) neighbourhood of the median. Any point outside this neighbourhood cannot possibly be the closest split pair.</p> <p>Traverse through points ahead of the top-most point. While you traverse downward, compare each point to the \\(7\\) points ahead of it. Find the situation with the minimum distance in such a scenario.</p>"},{"location":"Divide%20and%20Conquer/Minimum%20Distances/#why-7","title":"Why 7?","text":"<p>Take an arbitrary point \\(p\\). Assume there is at least one point ahead of it in a \\(\\delta\\) neighbourhood. Now draw 8 boxes of width and height \\(\\delta/2\\) below \\(p\\). Anything outside these boxes is further than \\(\\delta\\) away from p, and thus cannot be a candidate point. Furthermore, there can be only as many as 7 other points in these boxes. Any more and you have 2 in the same box, which gives us a distance between those two points of no more than \\(\\delta/\\sqrt2\\) .</p> <p>Suppose there is a point \\(q\\) less than \\(\\delta\\) away from \\(p\\) . This point must lie in one of the eight boxes. The maximum number of points we can fit between \\(q\\) and \\(p\\) is seven. Any more, and the closest point pair wil no longer be \\(p\\) and \\(q\\), but two of the internal points.</p>"},{"location":"Divide%20and%20Conquer/Minimum%20Distances/#time-complexity","title":"Time Complexity","text":"<p>Sorting Points &gt; \\(O(n.logn)\\) Closest Split Pair &gt; \\(O(n)\\)</p> <p>Recurrence &gt; \\(T(n) = 2T(n/2) + O(n)\\) Solution &gt; \\(O(n.logn)\\) from Master's Theoreom</p>"},{"location":"Divide%20and%20Conquer/Minimum%20Distances/#psuedo-code","title":"Psuedo-Code","text":"<pre><code>ClosestSplitPair(Q, R):\n    select all points in delta-neighbourhood of divider\n    start with top-most point\n    min_dist = really_really_large\n    for point S in set:\n        calculate distance with 7 points ahead of S as T\n        if any of these &lt; min_dist:\n            min_dist = this\n    return min_dist \n\nClosestPair(P):\n    start with point set P\n    sort in X and Y to get Px and Py\n    divide P by median of Px, making Q and R\n    get Qx, Qy, Rx, and Ry\n\n    pair_q = ClosestPair(Q)\n    pair_r = ClosestPair(R)\n    pair_split = ClosestSplitPair(Q, R)\n    return closest(pair_q, pair_r, pair_split)\n\n</code></pre>"},{"location":"Divide%20and%20Conquer/Recurrences%2C%20Master%20Theorem/","title":"Recurrences, Master Theorem","text":""},{"location":"Divide%20and%20Conquer/Recurrences%2C%20Master%20Theorem/#input-size","title":"Input Size","text":"<p>The input size describes the amount of space required to describe or set up the problem. It is proportional to the amount of bits needed to store the input. The input size is the only parameter that is needed to predict Time / Space Complexity for deterministic algorithms.</p>"},{"location":"Divide%20and%20Conquer/Recurrences%2C%20Master%20Theorem/#divide-and-conquer","title":"Divide and Conquer","text":"<p>Divide and Conquer algorithms come in three steps</p> <p>1) Divide the problem into smaller (typically disjoint) subproblems 2) Assume that you have already solved these subproblems below a certain layer 3) With this help from the recursion fairy, combine these solutions to get the solution of the current layer</p>"},{"location":"Divide%20and%20Conquer/Recurrences%2C%20Master%20Theorem/#recurrence","title":"Recurrence","text":"<p>A recurrence describes the runtime of an input size of \\(n\\) in terms of the same algorithm applied to smaller input sizes, and some additional overhead (the combine step).</p> <p>Example: for Karatsuba multiplication</p> \\[ \\begin {align} \\quad &amp;T(n) = 4T \\left(\\frac{n}{2} \\right) + c_1 \\cdot n \\\\ &amp;T(1) = c \\end {align} \\] <p>\\(T(1)\\) describes the base case of the algorithm</p>"},{"location":"Divide%20and%20Conquer/Recurrences%2C%20Master%20Theorem/#master-theorem","title":"Master Theorem","text":"<p>Define the recurrence as</p> \\[ \\quad T(n) = a \\cdot T(n/b) + c\\cdot n^d \\] <p>The master theorem gives a quick way to solve simple recurrences like this</p> \\[ \\quad T(n) =  \\begin {cases}     O(n^d \\log n)&amp; a = b^d\\\\     O(n^d)&amp; a &lt; b^d\\\\     O(n^{\\log_b a})&amp; a &gt; b^d\\\\ \\end {cases} \\]"},{"location":"Divide%20and%20Conquer/Recurrences%2C%20Master%20Theorem/#loose-proof","title":"Loose Proof","text":"<p>The number of levels in the recursion tree - \\(\\log_b n\\)</p> <p>The number of subproblems at level \\(j\\) -  \\(a^j\\)</p> <p>The subproblem size at level \\(j\\) - \\(n/b^j\\) </p> <p>Combine Phase at level \\(j\\) : </p> \\[\\quad a^j \\cdot \\left(\\frac{n}{b^j}\\right)^d \\cdot c\\] <p>So the total work is</p> \\[\\quad \\sum_{j=0}^{\\log_b(n)} n^d \\cdot c \\cdot \\left(\\frac{a}{b^d}\\right)^j\\] <p>This is a geometric progression. If \\(a=b^d\\) , the sum evaluates to \\(O(n^d \\log(n))\\) </p> <p>If \\(a &lt; b^d\\), this can be totalled as an infinite sum.</p> \\[\\quad = n^d \\cdot c \\cdot \\left(\\frac{1}{1-\\frac{a}{b^d}}\\right) = O(n^d)\\] <p>If \\(a&gt;b^d\\), we can't do the same because the ratio is &gt; 1</p> \\[\\quad=n^d \\cdot c \\cdot \\left(\\left(\\frac{a}{b^d}\\right)^{\\log_b n}-1\\right) = O(n^{\\log_ba})\\]"},{"location":"Divide%20and%20Conquer/Selection/","title":"Selection","text":"<p>The Goal of this is to select the i'th smallest number in an unsorted array.</p>"},{"location":"Divide%20and%20Conquer/Selection/#motivation","title":"Motivation","text":"<p>Calculating the \\(n'th\\) place of winners in a race with unsorted arrays of time, or finding the median from disorganised data.</p>"},{"location":"Divide%20and%20Conquer/Selection/#basic-solution","title":"Basic Solution","text":"<p>Pick a pivot element \\(p\\). Partition the array around \\(p\\). Push elements smaller than \\(p\\) to its left and larger than \\(p\\). Suppose the desired position is \\(i\\), and the median ends up at \\(j\\) after arranging the partitions.</p> <p>If \\(i = j\\) , then the \\(p\\) is in fact, the desired element. If \\(i &gt; j\\) , then \\(p\\) must be to the left of the desired element. So, we will scan to the right of \\(p\\). If \\(i &lt; j\\), then \\(p\\) must be to the right of the desired element. So, we will scan to the left of \\(p\\).</p> <p>In the ideal scenario, \\(p\\) will be the median, allowing us to shorten our search group to half size with \\(c.n\\) workcload. This will give us a recurrence relation of </p> \\[T(n) = c.n + T(n/2)\\] <p>which evaluates to \\(O(n)\\). Great! Our job's done, now all we need to do is find the median... uh-oh.</p>"},{"location":"Divide%20and%20Conquer/Selection/#the-paradox","title":"The Paradox","text":"<p>Finding the median in \\(O(n)\\) time is something that we've taken for granted, and it is unfortunately a non-trivial algorithm. But it is possible! To do so though, we need an algorithm that uses a value other than the median for a pivot.</p>"},{"location":"Divide%20and%20Conquer/Selection/#the-new-median-algorithm","title":"The New Median Algorithm","text":"<p>There exists a median of \"good enough\" quality, which gives a 30-70 split in the worst-case scenario. If we use this pivot for partitioning, we can find the \\(n/2'th\\) element (i.e. the median) in \\(O(n)\\) time.</p> <p>Step 1 -&gt; Group \\(A\\) into chunks of five. Conduct matches among these which end in \\(O(n)\\) time roughly.  Step 2 -&gt; The winners of these matches, i.e. the individual medians, are taken to the next round. Step 3 -&gt; Find the actual median of these winners. This will give us the final pivot. Use this pivot to partition \\(A\\), and calculate the actual median with this pivot.</p>"},{"location":"Divide%20and%20Conquer/Selection/#time-complexity","title":"Time Complexity","text":"\\[T(n) = c.n + T(n/5) + T(7n/10)\\] <p>The rearrangement across partition element, and calculating the sub-medians takes \\(c.n\\) time. \\(T(n/5)\\) is taken to calculate the median of the sub-medians. A further \\(T(7n/10)\\) is used to search for the desired element in the remaining array.</p> <p>This evaluates to a final complexity of \\(O(n)\\), which is derived purely by guessing.</p>"},{"location":"Divide%20and%20Conquer/Selection/#why-70","title":"Why 70%?","text":"<p>![[Pasted image 20220303212940.png]] This table taken from wikipedia shows that the MoM (median of medians) is well.. smack in the middle here. But consider the worst case scenario. It has to be larger than the 2nd and 1st element in at least half the sets, because it's larger than half the sub-medians. This means its larger than at least 30% of the sample space. For a similar reason, it must be smaller than at least 30% of the sample space.</p>"},{"location":"Dynamic%20Programming/Balls%20Maximisation/","title":"Balls Maximisation","text":""},{"location":"Dynamic%20Programming/Balls%20Maximisation/#problem-statement","title":"Problem Statement","text":"<p>Say you have an array of balls with weights</p> \\[[2, 5, 6, 4, 3, 5]\\] <p>You aren't allowed to pick adjacent balls. The goal is to maximise the weight of balls that you choose.</p>"},{"location":"Dynamic%20Programming/Balls%20Maximisation/#greedy-approach","title":"Greedy Approach","text":"<p>Pick the ball with the maximum weight, unless you have picked any of its neighbours. Repeat till no other ball can be picked.</p> <p>This seems (somewhat) sound, but can be disproved quickly via counterexample.</p>"},{"location":"Dynamic%20Programming/Balls%20Maximisation/#recursive-approach","title":"Recursive Approach","text":"<p>The goal of the recursive approach is to break this problem into subproblems, such that an inner subproblem should never have to rely on the output of an outer subproblem.</p>"},{"location":"Dynamic%20Programming/Balls%20Maximisation/#problem-definition","title":"Problem Definition","text":"<p>\\(B_i\\) := the i'th ball \\(W_i\\) := the weight of the i'th ball \\(S_i\\) := the optimal selection of balls up till the i'th ball \\(\\Omega_{\\,i}\\) := the optimal weight possible from selecting balls \\(B_1\\) through \\(B_i\\)</p> <p>The desired solution is \\(\\Omega_{\\,n}\\)</p>"},{"location":"Dynamic%20Programming/Balls%20Maximisation/#recurrence-relation","title":"Recurrence Relation","text":"<p>Consider the series </p> \\[\\quad B_1, B_2, B_3, B_4, ... B_{n-1}, B_n\\] <p>\\(B_i\\) may or may not be a part of \\(S_i\\). In the event that it is, \\(B_{i-1}\\) cannot be a part of \\(S_i\\). So, the optimal weight becomes</p> \\[\\quad W_i  + \\Omega_{\\,i-2}\\] <p>In the other case, \\(B_i\\) is not a part of \\(S_i\\) ,anything from \\(B_1\\) to \\(B_{i-1}\\) may be included in \\(S_i\\). However, this means that \\(S_i\\) and \\(S_{i-1}\\) must be the same (by definition). This gives us the final weight as</p> \\[\\quad \\Omega_{i-1}\\] <p>We have the power to choose from either of these cases. And because we want the maximum weight overall, we will choose maximum of the two possibilities. This gives us the optimal solution as</p> \\[ \\quad \\Omega_i = max     \\begin {cases}         W_i + \\Omega_{i-2} &amp; \\; B_i \\; selected\\\\         \\Omega_{i-1} &amp; \\; B_i \\; not\\  selected\\\\     \\end {cases} \\]"},{"location":"Dynamic%20Programming/Balls%20Maximisation/#recursive-algorithm","title":"Recursive Algorithm","text":"<p>Base Case: if i=1, \\(\\Omega_i\\) = \\(W_i\\) Recursive Case: if i &gt; 1, \\(\\Omega_i\\) = \\(max(W_i + \\Omega_{i-2},\\; \\Omega_{i-1})\\)  Starting: \\(\\Omega_n\\)</p>"},{"location":"Dynamic%20Programming/Balls%20Maximisation/#runtime","title":"Runtime","text":"<p>The runtime is observed as \\(O(2^n)\\), which honestly sucks.</p>"},{"location":"Dynamic%20Programming/Balls%20Maximisation/#memoisation","title":"Memoisation","text":"<p>With memoisation, we can boost the runtime to \\(O(n)\\), which is honestly epic. However, this means we also needs \\(O(n)\\) space alongside it.</p>"},{"location":"Dynamic%20Programming/Balls%20Maximisation/#iterative-algorithm","title":"Iterative Algorithm","text":"<p>The iterative approach converts this recursive algorithm to an iterative one.</p> <p>for each \\(i\\)'th ball from left to right,     \\(SelectUpto(i)\\)</p> <p>Where \\(SelectUpto\\) is the same as that of the recursive algorithm.</p>"},{"location":"Dynamic%20Programming/Balls%20Maximisation/#reconstruction","title":"Reconstruction","text":"<p>A ball is only added to the set if Case I is achieved, i.e. if it's better to add it.</p> <p>so we can modify the algorithm as such,</p> <pre><code>S = []\ni = n\nwhile i &gt;= 1:\n    if maxw[i] = maxw[2] + W[i]:\n        add B[i] to S\n        i -= 2\n    else:\n        i -= 1\n</code></pre>"},{"location":"Dynamic%20Programming/Knapsack/","title":"Knapsack","text":""},{"location":"Dynamic%20Programming/Knapsack/#problem-statement","title":"Problem Statement","text":"<p>Say you have a knapsack of size \\(W\\) (\\(W \\in I, W &gt; 0\\)). You have \\(n\\) indivisible items to put in that knapsack. Each item has a weight of \\(w_i\\) , and value of \\(v_i\\) . Your goal is to maximise the value inside the knapsack, without overfilling it.</p>"},{"location":"Dynamic%20Programming/Knapsack/#greedy-approach","title":"Greedy Approach","text":"<p>Start by picking the ball with highest value of \\(v_i / w_i\\) . Repeat this until the knapsack is filled.</p> <p>While this approach seems intuitive, it may fail. </p> <p>Consider this example</p> \\(w_i\\) \\(v_i\\) \\(v_i/w_i\\) 1 2 2 100 199 1.99 <p>\\(W = 100\\)</p> <p>in this case, the greedy approach starts by picking the first item. Then it picks one of the remaining items. This gives us a final value of 2 in the knapsack. Meanwhile, by picking the last item, one can get a value of 199. This can be extended to get a ratio between the optimal and greedy value as high as possible.</p>"},{"location":"Dynamic%20Programming/Knapsack/#recursive-approach","title":"Recursive Approach","text":"<p>\\(I_i\\) := the \\(i'th\\) item \\(w_i\\) := weight of the \\(i'th\\) item \\(v_i\\) := value of the \\(i'th\\) item \\(W\\) := maximum weight in knapsack \\(w\\) := weight of knapsack in subproblem \\(vmax(i, w)\\) := optimal solution for given value of \\(i\\) and \\(w\\)</p> <p>Consider the final state in which we have every optimal item in the knapsack. Now let us follow the steps of this approach in reverse. </p> <p>Say the last item was in the knapsack. So we remove this item. Now the knapsack is optimal for a weight up till \\(W - w_n\\) , and can only contain items between \\(1\\) and \\(n-1\\).</p> <p>Otherwise, the last item cannot be in the knapsack. So, the knapsack must contain items between \\(1\\) and \\(n-1\\), and must be optimal up to a weight of \\(W\\). </p> <p>Now we end up in the same state as before. We can repeat this till we get to any \\(i'th\\) item. So, the recurrence relation for the \\(i'th\\) item is given as</p> \\[ vmax(i, w) = max \\begin {cases}     vmax(i-1, w-w_i) + v_i &amp; \\quad I_i \\; chosen \\\\     vmax(i-1, w) &amp; \\quad I_i\\; not\\ chosen\\\\     \\end {cases} \\]"},{"location":"Dynamic%20Programming/Knapsack/#recursive-algorithm","title":"Recursive Algorithm","text":"<p>Base Cases:  * for \\(w=0\\), \\(vmax =0\\) {there is no space to add elements} * for \\(i = 0\\), \\(vmax = 0\\) {there are no elements to add}</p> <p>Recursive Case: * \\(vmax(i,w) = max(vmax(i-1, w-w_i) + v_i, vmax(i-1, w))\\) </p> <p>Starting: * \\(vmax(n, W)\\) </p>"},{"location":"Dynamic%20Programming/Knapsack/#runtime","title":"Runtime","text":"<p>This algorithm has a runtime of \\(O(2^n)\\), which again kinda sucks.</p>"},{"location":"Dynamic%20Programming/Knapsack/#memoisation","title":"Memoisation","text":"<p>With the power of memoisation, we can reduce the runtime to \\(O(nW)\\) {which needn't necessarily be better than \\(2^n\\), but it'll mostly be}.</p> <p>This is referred to as a pseudopolynomial runtime.</p> <p>However, we will need \\(O(nW)\\) space to use memoisation. This isn't always desirable.</p>"},{"location":"Dynamic%20Programming/Knapsack/#runtime-demonstration","title":"Runtime Demonstration","text":"I w v 1 4 3 2 3 2 3 2 4 4 3 4 <p>\\(W=6\\)</p> i\\w 0 1 2 3 4 5 6 0 0 0 0 0 0 0 0 1 0 0 0 0 3 3 3 2 0 0 0 2 3 3 3 3 0 0 4 4 4 6 7 4 0 0 4 4 4 8 8 ___ ## Iterative Approach An iterative approach to this problem just focuses on filling the table. We can traverse row major or column major. <pre><code>for w = 0 to W:\n    vmax[0, w] = 0\nfor i = 1 to n:\n    for i = 0 to W:\n        if (wt[i] &gt; w):\n            vmax[i, w] = vmax[i-1, w]\n        else:\n            vmax[i, w] = max(vmax[i-1, w], vmax[i-1, w - wt[i]] + v[i])\nreturn vmax[n, W]\n</code></pre>"},{"location":"Dynamic%20Programming/Matrix%20Chaining/","title":"Matrix Chaining","text":""},{"location":"Dynamic%20Programming/Matrix%20Chaining/#problem-definition","title":"Problem Definition","text":"<p>Given a set of \\(n\\) matrices {\\(A_i\\)}, the goal of this algorithm is to multiply matrices in the optimal order, to minimise number of computations required.</p>"},{"location":"Dynamic%20Programming/Matrix%20Chaining/#motivation","title":"Motivation","text":"<p>Multiplying matrices quickly is obviously important for many mathematical tools (cough cough, linear algebra), but the applications of this algorithm are far beyond that. Anything which involves optimally combining \\(n\\) elements, with known costs / benefits of combining \\(k\\; (&lt; n)\\) elements can utilise this algorithm.   </p>"},{"location":"Dynamic%20Programming/Matrix%20Chaining/#subproblem-definition","title":"Subproblem Definition","text":"<p>\\(\\Omega(X)\\) = Optimal cost for computing value of \\(X\\) (contiguous) matrix set. \\(\\sigma(X, Y)\\) = Cost of multiplying the result of \\(X\\) matrix set and \\(Y\\) matrix set. Equivalent to \\(rows(X_1) \\cdot cols(Y_y)\\)</p>"},{"location":"Dynamic%20Programming/Matrix%20Chaining/#recurrence","title":"Recurrence","text":"\\[\\Omega(A) = min(\\Omega(X) + \\Omega(X') + \\sigma(X,X'))\\] <p>provided \\(X\\) and \\(X'\\) constitute the optimal breakdown of matrix set \\(A\\), assuming \\(X\\) ends at element \\(A_k\\). We can do this by considering every breakdown, and taking the minimum of those. </p> <p>Base Case: \\(\\Omega(M) = 0\\) if \\(|M| = 1\\)</p>"},{"location":"Dynamic%20Programming/Matrix%20Chaining/#complexity","title":"Complexity","text":"<p>The DP table has a size of \\(n^2\\), and filling each cell takes a time of \\(O(n)\\). This gives us a total complexity of \\(O(n^3)\\). The space complexity is proportional to the size of the table, which is just \\(O(n^2)\\).  Note that we only fill the upper triangle of the DP table.</p>"},{"location":"Dynamic%20Programming/Matrix%20Chaining/#pseudo-code","title":"Pseudo-Code","text":"<pre><code>for i form 1 to n:\n    DP[i, i] = 0 \nfor length from 1 to n-1:\n    for i from 1 to n-length:\n        j = i + length\n        tot_min = 1000000000\n        for k from i+1 to j:\n            DP[i, j] = min{tot_min, DP[i, k] + DP[k+1, j] + sigma(k)}\nreturn DP[1, n]\n</code></pre>"},{"location":"Dynamic%20Programming/Sequence%20Alignment/","title":"Sequence Alignment","text":""},{"location":"Dynamic%20Programming/Sequence%20Alignment/#edit-distance","title":"Edit Distance","text":"<p>The Needleman-Wunsch score defines the edit distance as the sum of 'cost' of mismatches and the 'cost' of gaps.</p>"},{"location":"Dynamic%20Programming/Sequence%20Alignment/#motivation","title":"Motivation","text":"<p>Edit distance is useful for finding the difference between two strings. This is helpful in using <code>diff</code> operations, autocorrect, speech recognition, and computational biology.</p>"},{"location":"Dynamic%20Programming/Sequence%20Alignment/#problem-definition","title":"Problem Definition","text":"<p>We are given two strings, \\(X = [x_i]\\) and \\(Y = [y_i]\\)  of lengths \\(m\\) and \\(n\\) respectively. The goal is to find the insertion of gaps so as to achieve the minimum edit distance for these two strings.</p> <p>\\(\\Omega(A, B)\\) = optimal solution for strings \\(A\\) and \\(B\\).</p>"},{"location":"Dynamic%20Programming/Sequence%20Alignment/#subproblem-definition","title":"Subproblem Definition","text":"<p>Let's look at the end of these strings. We have three possibilities. One is that they're both perfectly aligned, the other two involve one of them ending in a gap. Let us define the ideal strings (gaps included) as \\(X'\\) and \\(Y'\\), of lenths \\(m'\\) and \\(n'\\)  respectively. We may end up with three cases:</p> <p>1) \\(m = n\\) =&gt; \\(\\Omega(X,Y) = f(\\Omega(X-x_m, Y-y_n))\\)  2) \\(m &gt; n\\)  =&gt; \\(\\Omega(X,Y) = f(\\Omega(X-x_m, Y))\\) 3) \\(m &lt; n\\)  =&gt; \\(\\Omega(X,Y) = f(\\Omega(X, Y-y_n))\\)</p>"},{"location":"Dynamic%20Programming/Sequence%20Alignment/#proof-of-optimal-substructure","title":"Proof of Optimal Substructure","text":"<p>Claim: \\(\\Omega(X,Y)\\) with \\(x_m, y_n\\) removed = \\(\\Omega(X-x_m, Y-y_n)\\).</p> \\[\\quad Let \\ \\Omega (X, Y) = \\Omega (X-x_m, Y-y_n) + \\alpha (x_m, y_n)\\] <p>If there is any other string \\(\\overline{X}\\) , \\(\\overline{Y}\\) that gives \\(\\Omega(\\overline{X},\\overline{Y}) &gt; \\Omega(X-x_m, Y-y_m)\\), that would imply that \\(\\Omega(\\overline{X}, \\overline{Y}) + \\alpha (x_m, y_m)\\) &gt; \\(\\Omega(X, Y)\\), which would violate the optimality of the solution. Therefore, no such string is possible.</p>"},{"location":"Dynamic%20Programming/Sequence%20Alignment/#recurrence-relation","title":"Recurrence Relation","text":"<p>Define \\(A_i\\) as the string \\(A\\) up till \\(i\\) (included)</p> \\[ \\quad \\Omega(X_i, Y_j) = min     \\begin{cases}     \\Omega(X_{i-1}, Y_{j-1}) + \\alpha(x_i, y_j) \\\\      \\Omega(X_{i-1}, Y_j) + \\alpha(gap) \\\\     \\Omega(X_i, Y_{j-1}) + \\alpha(gap) \\\\     \\end{cases} \\] <p>Base Case: \\(\\(\\begin{align}   \\quad &amp; \\Omega(X_i, 0) = i \\cdot \\alpha(gap)\\\\         &amp; \\Omega(0, Y_j) = j \\cdot \\alpha(gap) \\end{align}\\)\\)</p>"},{"location":"Dynamic%20Programming/Sequence%20Alignment/#complexity","title":"Complexity","text":"<p>This gives us time and space complexities of \\(\\Theta(mn)\\) , when we memoise the DP table.</p>"},{"location":"Dynamic%20Programming/Sequence%20Alignment/#pseudo-code","title":"Pseudo-Code","text":"<pre><code>Define P[m+1, n+1]\nAlign (X, Y):\n    for each i: P[i, 0] = i.alpha(gap)\n    for each j: P[0, j] = j.alpha(gap)\n    for i from 1 to m:\n        for j from 1 to n:\n            P[i, j] = min(P[i-1, j-1] + alpha(x[i], y[j]),\n                          P[i, j-1] + alpha(gap)\n                          P[i-1, j] + alpha(gap)\n                          )\n    return P[m, n]\n</code></pre>"},{"location":"Dynamic%20Programming/Sequence%20Alignment/#space-optimisation","title":"Space Optimisation","text":"<p>Each row only needs the values in the current row, and the row above it to fill (provided we are filling the table in row major). In this scenario, we choose to only store the current row and the row above it. This improves our time complexity to \\(O(min(m, n))\\).</p>"},{"location":"Graph%20Algorithms/Bellman-Ford%20Algorithm/","title":"Bellman Ford Algorithm","text":""},{"location":"Graph%20Algorithms/Bellman-Ford%20Algorithm/#problem-statement","title":"Problem Statement","text":"<p>Imagine a directed graph, except now it has negative length edges too. Dijkstra's algorithm is no longer sufficient for this purpose. </p>"},{"location":"Graph%20Algorithms/Bellman-Ford%20Algorithm/#observation","title":"Observation","text":"<p>This problem may not have a defined solution in the first place. In the presence of a negative cycle, there will be no longest path connecting a source to its destination.</p>"},{"location":"Graph%20Algorithms/Bellman-Ford%20Algorithm/#approach","title":"Approach","text":"<p>Bellman-Ford handles this problem with a Dynamic Programming approach. The optimal path to a point is defined with both the terminal point, and the maximum number of edges in the path.</p>"},{"location":"Graph%20Algorithms/Bellman-Ford%20Algorithm/#pseudo-code","title":"Pseudo-Code","text":"<pre><code>define dp table of size V x V.\nset dp of all lengths pointing to the source as 0.\nset dp of zero length pointing to any vertex as 0.\n\nfor each (length i) from (1 to n-1),                   - O(V^3)\n    for each (vertex v),                               - O(V^2) \n        for each edge going into v (parent vertex u),  - O(V) \n            dp[v, i] = min(dp[u, i-1] + l[u, v]).      - O(1)\n</code></pre>"},{"location":"Graph%20Algorithms/Bellman-Ford%20Algorithm/#runtime-analysis","title":"Runtime Analysis","text":"<p>This algorithm has a runtime of \\(O(VE)\\). </p>"},{"location":"Graph%20Algorithms/Bridge%20Edge%20Detection/","title":"Bridge Edge Detection","text":""},{"location":"Graph%20Algorithms/Bridge%20Edge%20Detection/#problem-definition","title":"Problem Definition","text":"<p>Consider the following graph</p> <pre><code>graph LR;\n    1((1));\n    2((2));\n    3((3));\n    4((4));\n    5((5));\n    6((6));\n    7((7));\n    1 --- 2\n    2 --- 3\n    4 --- 5\n    3 --- 6\n    2 --- 6\n    2 --- 4\n    4 --- 7\n    5 --- 7 \n</code></pre> <p>In this case, if we remove the edge \\(2-4\\), we get two disconnected components. Therefore, \\(2-4\\) counts as bridge edge. However, \\(2-6\\) does not do this, so it's not a bridge edge.</p> <p>We define a graph as 2-Edge Connected, If and only if it has no bridge edge. The problem, now, is to find whether or not a graph is 2EC.</p>"},{"location":"Graph%20Algorithms/Bridge%20Edge%20Detection/#observation","title":"Observation","text":"<ul> <li>Consider you are traversing \\(2-4\\). We know that it is a bridge edge. We can remark that there are no edges moving up from the subraph below \\(4\\) to the subgraph above \\(2\\). If that were true, the edge would no longer be a bridge edge, because there would be two edges connecting the subgraph to \\(1\\).</li> <li>Additionally, we only need to check backward edges, as a DFS tree is incapable of having cross edges.</li> <li>All we need to know about a subgraph to detect the bridge edge is the highest back-edge coming out of it.</li> </ul>"},{"location":"Graph%20Algorithms/Bridge%20Edge%20Detection/#pseudocode","title":"Pseudocode","text":"<pre><code>define arrays for visited flags and visit time\ndefine time as 0\ndefine HBED (highest back edge destination) as 0\n\ncheck-2EC([vertex v] -&gt; HBED s) {\n    mark v as visited.\n    set time[v] and HBED as t++.\n\n    for each (vertex w) in adjacency list of (v),\n        if w is unvisited,\n            reduce HBED to check-2EC(w) or pass.\n        else,\n            reduce HBED to depth of w or pass.\n\n    if HBED is below or at v,\n        report bridge edge and terminate.\n    else, \n        report success.\n}\n\n</code></pre>"},{"location":"Graph%20Algorithms/Bridge%20Edge%20Detection/#runtime","title":"Runtime","text":"<p>As this algorithm is a small modification on depth-first traversal, it retains the same runtime of \\(O(E + V)\\)</p>"},{"location":"Graph%20Algorithms/Depth%20First%20Search/","title":"Depth First Search","text":""},{"location":"Graph%20Algorithms/Depth%20First%20Search/#problem-definition","title":"Problem Definition","text":"<p>Consider the following undirected graph.</p> <pre><code>graph LR;\n    1((1));\n    2((2));\n    3((3));\n    4((4));\n    5((5));\n    6((6));\n    7((7));\n    1 --- 2\n    2 --- 3\n    1 --- 3\n    4 --- 5\n    3 --- 6\n    6 --- 7\n    4 --- 6\n    2 ---- 6\n    2 --- 4\n    4 --- 7\n</code></pre> <p>Say you want to go from \\(1\\) to \\(7\\). You want a traversal that will reliably and efficiently find this path.</p>"},{"location":"Graph%20Algorithms/Depth%20First%20Search/#algorithm-definition","title":"Algorithm Definition","text":"<p>The algorithm goes from the source, and picks nodes randomly. It doesn't go to any node twice, and backtracks when it reaches a dead end. Eventually, it will approach the desired node (provided the source and destination are connected to each other).</p>"},{"location":"Graph%20Algorithms/Depth%20First%20Search/#observations","title":"Observations","text":"<ul> <li>Since each node is visited only once, we can model the path that the algorithm travels over as a tree. This will be referred to as a DFS Tree.</li> <li>There are no cross-edges possible. If subtree \\(S1\\) and \\(S2\\) have no forward edges connecting them, this means that there is no edge present between \\(S1\\) and \\(S2\\) (otherwise, it would have been visited before backtracking). Therefore, any edge besides a forward edge is a back edge.</li> </ul> <pre><code>graph LR\n    1((1));\n    2((2));\n    3((3));\n    4((4));\n    5((5));\n    6((6));\n    7((7));\n    1 --- 2\n    1 --- 3\n    3 --- 6\n    6 --- 7\n    4 --- 5\n    2 --- 4;\n    2 -.- 6\n    2 -.- 3\n    4 -.- 6\n</code></pre> <p>For example, one might think a graph with dotted edges as cross-edges here works. But, the algorithm must traverse \\(3\\) and \\(6\\) before backtracking from \\(2\\), so it is invalid.</p>"},{"location":"Graph%20Algorithms/Depth%20First%20Search/#pseudo-code","title":"Pseudo-Code","text":"<pre><code>make an array with 'visited' flag for all nodes\nmake an array for visiting time of all nodes\nstart time t as 0\n\nDFS(node v){\n    mark node v as visited\n    set time of visit as t\n    increment t\n    for every vertex w in adjacency list of v {\n        if w is not visited {\n            perform DFS(w)\n        }\n    }\n}\n</code></pre>"},{"location":"Graph%20Algorithms/Depth%20First%20Search/#runtime","title":"Runtime","text":"<ul> <li>Each node has to be visited at least once before the algorithm ends.</li> <li>Each edge need not be visited before the algorithm ends. However, since we are using an adjacency list, each edge is checked twice.</li> </ul> <p>This puts the total runtime as \\(O(V + E)\\)</p>"},{"location":"Graph%20Algorithms/Dijkstra%27s%20Algorithm/","title":"Dijkstra's Algorithm","text":""},{"location":"Graph%20Algorithms/Dijkstra%27s%20Algorithm/#problem-statement","title":"Problem Statement","text":"<p>The goal of Dijkstra's algorithm is to find all the shortest paths connecting a source vertex \\(S\\), to every vertex in a directed graph. It can also be used to calculate just one Source-Destination pair efficiently.</p>"},{"location":"Graph%20Algorithms/Dijkstra%27s%20Algorithm/#approach","title":"Approach","text":"<p>We will divide the graph into two components, \\(S\\) and \\(S'\\). In \\(S\\), we store all the points whose shortest distance from the origin we know of. The goal will be to expand \\(S\\) till eventually it includes every point in the graph.</p>"},{"location":"Graph%20Algorithms/Dijkstra%27s%20Algorithm/#observations","title":"Observations","text":"<ul> <li>If we know the distance to every point in some region \\(S\\), we can tell with certainty the shortest distance to one and only one point in \\(S'\\) with absolute certainty. </li> <li>This point is the one with the minimum value of shortest distance to any point in \\(S\\), i.e. the closest point to \\(S\\).</li> <li>Any other point may have a shorter path to it from \\(S\\), which flows through this optimal path. </li> <li>At the start, the optimal path is only known for the source.</li> </ul>"},{"location":"Graph%20Algorithms/Dijkstra%27s%20Algorithm/#pseudo-code","title":"Pseudo-Code","text":"<pre><code>set distance of each vertex from source to +inf.\ncreate a min heap H with all vertices in, measured by their distance.\n\nwhile H is not empty,\n    frontier vertex f = delete_min(H).           - O(V. log V)\n    for all outgoing edges from (fv) to S',      - O(E. log V)\n        relax d[v] till d[f] + l[fv].\n        update the heap to reflect this change.  - O(log V)\n</code></pre>"},{"location":"Graph%20Algorithms/Dijkstra%27s%20Algorithm/#runtime","title":"Runtime","text":"<p>We run heapify every time a point is moved into the frontier. As this happens no more than \\(V\\) times, the time taken by this is \\(O(V \\log V)\\). </p> <p>We check each edge once for potential relaxation and heap reset. So the runtime of this part is \\(O(E \\log V)\\). Therefore the total runtime is \\(O((E + V)log(V))\\) </p>"},{"location":"Graph%20Algorithms/Dijkstra%27s%20Algorithm/#proof-of-correctness","title":"Proof Of Correctness","text":"<p>This proof is performed via induction.</p> <p>At any iteration, suppose \\(f\\) is the frontier vertex, being moved from \\(S'\\) to \\(S\\). Then, d[w] is the shortest path length from \\(S\\) to \\(w\\) </p> <p>Base Case - \\(S = \\{s_0\\}\\)</p> <p>Induction Hypothesis - Suppose we have \\(S\\) of some arbitrary size, and are moving a vertex \\(f\\). </p> \\[d[f] = d[u] + l_{uw} \\quad [U \\in S]\\] <p>Suppose an alternate shorter path \\(P\\) exists. If it connects directly from \\(S\\) to \\(f\\), then it cannot be shorter than the existing path. If it connects through \\(S'\\), it connects through another vertex, which is further apart, making it longer than the path given by Dijkstra.</p>"},{"location":"Graph%20Algorithms/Kruskal%27s%20Algorithm/","title":"Kruskal's Algorithm","text":""},{"location":"Graph%20Algorithms/Kruskal%27s%20Algorithm/#problem-statement","title":"Problem Statement","text":"<p>The goal is to find the minimum spanning tree of the graph. Define the weight of the tree as the sum of weight of each edge. Then, the MST is the tree touching each edge which has the minimum weight.</p>"},{"location":"Graph%20Algorithms/Kruskal%27s%20Algorithm/#observations","title":"Observations","text":"<ul> <li>Divide the graph into two subgraphs, \\((S, V-S)\\). The lightest edge crossing between these two sub-graphs must be in the MST. If it is not, then we can generate a minimum spanning structure of smaller size by replacing it with the smallest such edge.</li> </ul>"},{"location":"Graph%20Algorithms/Kruskal%27s%20Algorithm/#pseudo-code","title":"Pseudo-Code","text":"<pre><code>sort edges in increasing order of weight.      -- O(E.log(E))\ndefine T as the set of all edges in the MST.\n\nfor each (edge e) with (i from 0 to E),        -- O(E)\n    if adding e to T does not make a cycle,   \n        add e to T.                            -- O(1), upto V times\n</code></pre> <p>This requires \\(V\\) Find / Union operations to add a point to the chart. We can perform a find union in \\(O(log^*n)\\) time with path compression technique, which makes the complexity basically \\(O(V)\\). Total complexity is given as \\(O(E.\\log(V))\\) </p>"},{"location":"Graph%20Algorithms/Kruskal%27s%20Algorithm/#find-union-algorithm","title":"Find-Union Algorithm","text":"<p>Basically, we link each element to it's \"parent\". For each chain, there is a ternimating element. We consider this as the parent of the graph component. If two elements share the same parent / leader, a cycle is formed. This can be evaluated very quickly.</p>"},{"location":"Graph%20Algorithms/Kruskal%27s%20Algorithm/#correctness","title":"Correctness","text":"<p>Correctness of Kruskal's algorithm can be proven by contradiction.</p> <p>Kruskal's algorithm produces a graph with no cycles, by definition of the algorithm. That is the first requirement.</p> <p>Secondly, Consider a situation with components \\(S\\) and \\(S'\\), and an edge \\(e\\) between them, given by Kruskal's algorithm. Assume there exists an edge \\(e'\\), such that picking \\(e'\\) over \\(e\\) gives a better outcome. Since the weights inside \\(S\\) and \\(S'\\) are independent of which edge is chosen, the difference comes only from this bridge edge. Since Kruskal's algorithm selects edges in ascending order of weight, if \\(e'\\) were truly lighter than \\(e\\), it would've been chosen first. </p>"},{"location":"Greedy%20Algorithm/Maximum%20Element%20Count/","title":"Maximum Element Count","text":""},{"location":"Greedy%20Algorithm/Maximum%20Element%20Count/#problem-definition","title":"Problem Definition","text":"<p>You are given \\(n\\) tasks, with starting and finishing times \\((s_i, f_i)\\). The goal now is to find the maximum number of non-overlapping tasks that can be performed by one worker.</p>"},{"location":"Greedy%20Algorithm/Maximum%20Element%20Count/#greedy-algorithm","title":"Greedy Algorithm","text":"<p>Sort [\\(t_i\\)] by Shortest Finishing Time First. Then pick the first element. Pick the next element if it doesn't overlap, or go to the one after if it does. Repeat this till you get to the last element.</p>"},{"location":"Greedy%20Algorithm/Maximum%20Element%20Count/#proof-via","title":"Proof via ???","text":"<p>TODO</p>"},{"location":"Greedy%20Algorithm/Maximum%20Element%20Count/#time-complexity","title":"Time Complexity","text":"<p>This algorithm has a time complexity of \\(O(n \\log n)\\), which is the time taken to sort these \\(n\\) tasks. The picking afterwards is done in \\(O(n)\\) time.</p>"},{"location":"Greedy%20Algorithm/Minimal%20Partitioning/","title":"Minimal Partitioning","text":""},{"location":"Greedy%20Algorithm/Minimal%20Partitioning/#problem-definition","title":"Problem Definition","text":"<p>You have been given \\(n\\) tasks, [\\(t_i\\)], just like last time. Each task has a starting time and a finishing time \\((s_i, f_i)\\). The goal is to find the minimum number of workers needed to perform these tasks without needing any amount of overlap.</p>"},{"location":"Greedy%20Algorithm/Minimal%20Partitioning/#motivation","title":"Motivation","text":"<p>A life-scale application is deciding the number of lecture halls needed to teach \\(n\\) courses.</p>"},{"location":"Greedy%20Algorithm/Minimal%20Partitioning/#greedy-algorithm","title":"Greedy Algorithm","text":"<p>Start by sorting the tasks by their starting times. After doing this, start with one worker. Give the first task to the first worker. If another task is started before the current one ends, assign it to another worker. Repeat this for all \\(n\\) tasks.</p>"},{"location":"Greedy%20Algorithm/Minimal%20Partitioning/#proof-via-lower-bound-for-the-optimal","title":"Proof via Lower Bound for the Optimal","text":"<p>Define the depth \\(d\\) of \\(T\\) as the maximum number of intervals active at any given time. The optimal number of workers needed must surely be equal to or more than the \\(d\\). We will prove that the greedy algorithm will never use more than \\(d\\) workers.</p> <p>Say, when we got to a task \\(t\\), there were \\(d\\) active workers already. So, the algorithm gave \\(t\\) to the \\(d+1th\\) worker. This means that there are \\(d+1\\) intervals overlapping with each other, so the depth must be \\(d+1\\). This is a direct contradiction, and thus not possible. </p>"},{"location":"Greedy%20Algorithm/Minimal%20Partitioning/#time-complexity","title":"Time Complexity","text":"<p>\\(O(n \\log n)\\) is the time complexity of sorting these \\(n\\) tasks. The picking afterwards is done in \\(O(n^2)\\) time, giving a total time complexity of \\(O(n^2)\\).</p>"},{"location":"Greedy%20Algorithm/Minimum%20Weighted%20Completion%20Time/","title":"Minimum Weighted Completion Time","text":""},{"location":"Greedy%20Algorithm/Minimum%20Weighted%20Completion%20Time/#problem-definition","title":"Problem Definition","text":""},{"location":"Greedy%20Algorithm/Minimum%20Weighted%20Completion%20Time/#minimum-total-completion-time","title":"Minimum Total Completion Time","text":"<p>We are given \\(n\\) tasks with lengths [\\(l_i\\)]. Our goal is to perform these tasks in an order such as to minimise the sum of finishing time for each task.</p>"},{"location":"Greedy%20Algorithm/Minimum%20Weighted%20Completion%20Time/#minimum-weighted-completion-time","title":"Minimum Weighted Completion Time","text":"<p>In this problem, each tasks also have a weight \\(w_i\\). we have to miminise weighted completion time where weighted completion time \\(\\overline{C_i} = w_i \\cdot C_i\\)</p>"},{"location":"Greedy%20Algorithm/Minimum%20Weighted%20Completion%20Time/#greedy-algorithm","title":"Greedy Algorithm","text":"<p>We sort the problems in order of \\(w_i\\,/\\,l_i\\) . Then we perform the tasks in that order. Now why would that give the optimal solution?</p>"},{"location":"Greedy%20Algorithm/Minimum%20Weighted%20Completion%20Time/#proof-via-exchange-argument","title":"Proof via Exchange Argument","text":"<p>Say we arrange the tasks in the ideal order, which we are saying is also the hypothesised optimal order. Call this order as \\(\\sigma = [t_1, t_2, ... t_n]\\) </p> <p>Now suppose we create an order \\(\\sigma^*\\). This order is totally different from \\(\\sigma\\), but must have at least one inversion between adjacent elements. (Lemma: Having an inversion between non-adjacent elements will also create an inversion between some adjacent elements. Proof is (decently) trivial). Let's call these inverted adjacent elements as \\(t_q\\) and \\(t_p\\), </p> <p>$$\\begin{align} &amp; \\overline{C}(\\sigma^) = \\left(\\sum_{i\\neq p,q} C_i \\cdot w_i\\right) + C^_p \\cdot w_p + C^*_q \\cdot w_q \\</p> <p>&amp; \\overline{C}(\\sigma') = \\left(\\sum_{i\\neq p,q} C_i \\cdot w_i\\right) + C'_p \\cdot w_p + C'_q \\cdot w_q \\\\</p> <p>&amp; \\overline{C}(\\sigma') - \\overline{C}(\\sigma^) \\&amp;= (C'_p - C^_p)\\cdot w_p + (C'_q - C^*_q)\\cdot w_q \\&amp;= -l_q \\cdot w_p + l_p \\cdot w_q &gt; 0  \\end{align}$$</p> <p>By this, we can deduce that \\(\\sigma'\\) is a better arrangement. This means that no matter which arrangement we pick besides \\(\\sigma\\), a better arrangement exists. This means that \\(\\sigma\\) is the optimal arrangement. </p>"},{"location":"Greedy%20Algorithm/Minimum%20Weighted%20Completion%20Time/#time-complexity","title":"Time Complexity","text":"<p>This algorithm has a time complexity of \\(O(n\\log n)\\), which is the time taken to sort these \\(n\\) tasks.</p>"}]}